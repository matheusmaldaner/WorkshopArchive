{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Content Produced by UF Signal Processing Society**\n",
    "\n",
    "**Authors: Raul Valle**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to GPU Accelerated Scientific Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name implies, we will learn how to manage CPU (host) and GPU(device) interactions for performing GPU accelerated computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I'd like to thank NVIDIA for creating CUDA, it's a super easy-to-learn API for using your GPU and accelerating computations. \n",
    "\n",
    "I'd like to thank UF and SPS for their interest in this student organization, giving everyone a platform to share their knowledge and resources.\n",
    "\n",
    "Finally, I'd like to personally thank Dr. Silva, Dr. Principe, Benjamin Colburn, Dr. Wong, Dr. Shea, Matheus Kunzler-Maldaner and Evan Partidas for their continual support in my academic journey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scientific Computing is increasingly important as topics such as data analysis, signal processing and machine learning gain relevance in research. \n",
    "\n",
    "Specifically, it's important for synthetic tests where evaluation data can be generated independently from other each-other while the computations are as parallelizable as possible.\n",
    "\n",
    "Another case in which it's important is in any real-time system which is computation heavy and operates in a sequential but parallelizable manner. (We will discuss this more thoroughly soon).\n",
    "\n",
    "The intent of this workshop is to introduce the user to basic libraries and APIs for GPU accelerating their own scripts, mainly by using the RAPIDS SDK for CuPy and the Numba API for GPU utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are actually very few pre-requisites for your understanding of using GPUs in your program. Potentially, having a Digital Design and Computer Architecture background will aid in your understanding of how to minimize latencies, however we will not discuss latencies in depth (if you are interested, be on the lookout for the *Hardware Accelerated Scientific Computing* and the *Introduction to Real Time Signal Processing* workshops TBA in the future!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, there are pre-requisites for following this workshop. At the very least, you should have a basic understanding of *Python* and if you don't, it still shouldn't be a major issue as it's almost psuedocode.\n",
    "\n",
    "To get started, we will need to get your environment set up. We will be using the conda package manager to install the necessary packages. If you don't have conda installed, you can download it from here: https://www.anaconda.com/download/success\n",
    "\n",
    "Now we need to actually setup the conda environment for the following workshop. Run the following commands (or their OS equivalent) in your terminal.\n",
    "\n",
    "```\n",
    "conda env create --name SPS\n",
    "conda activate SPS\n",
    "```\n",
    "\n",
    "Then, we need to install the relevant libraries for this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install ipykernel numba -c rapidsai -c nvidia -c conda-forge rapids=24.06 python=3.11 cuda-version=12.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your conda environment is selected as your jupyter kernel wherever that selection is made.\n",
    "\n",
    "Now we are ready to get to work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a section on CPU usage, specifically for computing. This section is relatively straight forward for the average programmer, but its purpose is to incentivize the usage of more scalable and efficient methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Regular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPUs or *Central Processing Units* are what your computer uses for basically everything. They run all commands on your computer at a very fast pace (CPU clocks operate between 2 and 5 GHz, and can perform at least 1 instruction per second or *IPS*). \n",
    "\n",
    "Assuming the bare minimum, that is, the instructions being run are of SISD type (*Single Input Single Data*), your computer can perform at least 2 Billion instructions per second which is a pretty large amount of operations for such a short period of time. Let's generate an example and time it to put things into perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List comprehension: 0.0021 seconds\n",
      "List operation: 0.0148 seconds\n",
      "Convolution: 0.0470 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "tic = time.perf_counter()\n",
    "x = [i for i in range(100_000)]\n",
    "toc = time.perf_counter()\n",
    "print(f\"List comprehension: {toc - tic:0.4f} seconds\")\n",
    "\n",
    "tic = time.perf_counter()\n",
    "y = [2*i+1 for i in x]\n",
    "toc = time.perf_counter()\n",
    "print(f\"List operation: {toc - tic:0.4f} seconds\")\n",
    "\n",
    "y = [2*i+1 for i in range(1_000)]\n",
    "tic = time.perf_counter()\n",
    "z = np.convolve(x, y)\n",
    "toc = time.perf_counter()\n",
    "print(f\"Convolution: {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While some simple operations perform relatively well, it is clear that more complex compuations (like Convolutions) can take over 10x as long.\n",
    "\n",
    "Moreover, ponder the situation if you further need to scale these operations to a larger array size, or potentially even having multiple dimensions (as when we work with images or videos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address the previous issues (from the CPU), CPU architectures were advanced to support vector-robust methods, or *Vectorized Instructions*. *Vectorized Instructions* are instructions with memory access patterns embedded into the instruction, that is that they are SIMD type (*Single Instruction Multiple Data*).\n",
    "\n",
    "By instead making use of these kinds of instructions, we can make significant gains in performance - however, there are downsides to this. Python is built on-top of C++, which means that to take advantage of these improvements there need to be strict implications about the data's memory access. The primary down-side is that Python Lists are made to handle objects and be mutable, whereas Vectorized-Access supported lists (which are associated with Numpy) must remain constant-sized and have a fixed data-type.\n",
    "\n",
    "Let's see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array creation: 0.0003 seconds\n",
      "Array operation: 0.0004 seconds\n",
      "Convolution: 0.0284 seconds\n"
     ]
    }
   ],
   "source": [
    "x = np.empty(100_000)\n",
    "y = np.empty(100_000)\n",
    "z = np.empty(199_999)\n",
    "\n",
    "tic = time.perf_counter()\n",
    "x = np.arange(100_000)\n",
    "toc = time.perf_counter()\n",
    "print(f\"Array creation: {toc - tic:0.4f} seconds\")\n",
    "\n",
    "tic = time.perf_counter()\n",
    "y = 2*x+1\n",
    "toc = time.perf_counter()\n",
    "print(f\"Array operation: {toc - tic:0.4f} seconds\")\n",
    "\n",
    "y = 2*np.arange(1_000)+1\n",
    "tic = time.perf_counter()\n",
    "z = np.convolve(x, y)\n",
    "toc = time.perf_counter()\n",
    "print(f\"Convolution: {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are some clear timing benefits of these implementations when directly compared, however by \"playing\" around with these numbers, these vectorized instructions are not very scalable and actually they fall short the larger the arrays are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a section on GPU usage, specifically for computing. This section dives into the most fundamental topics involved in this kind of programming, and displays the scalability of these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Note**: The code in this section involving GPU Device utility will often need to be ran twice. The reason for this is that the CPU needs to communicate to the GPU the *context* of the processing it will do. We will mention this again later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Hardware Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some brief insights on the Hardware architecture to give some insights into how your computer system will work.\n",
    "\n",
    "We mentioned that the CPU is the *Central Processing Unit* and this naming convention will be relevant as shown later. However, when it was introduced previously, some might be under the notion that the CPU is 1 unified piece of hardware, it is not.\n",
    "\n",
    "Actually, modern CPUs have 2 main configurations:\n",
    "\n",
    "*Single-Die, Multi-Core CPU* (SDMC CPU) or *Multi-Die, Multi-Core CPU* (MDMC CPU)\n",
    "\n",
    "There are tradeoffs with each design, most notably: \n",
    "\n",
    "SDMC CPUs: **less** threads, **faster** clock speed\n",
    "\n",
    "MDMC CPUs: **more** threads, **slower** clock speed\n",
    "\n",
    "Within each die are multiple cores, which each contain multiple threads. Each *core* can run its own program, however certain kinds of programs can take advantage of the individual *threads* in a core to improve computational efficiency. We won't get into the specifics of threading yet, we will reserve that for a future workshop -\n",
    "\n",
    "However, this should give some insight into the utility of a GPU. Originally named the *Graphics Processing Unit*, a GPU is really just a CPU optimized for vectorized operations.\n",
    "\n",
    "In this case, if we were to continue the comparisons above:\n",
    "\n",
    "MDMC CPUs: **more** threads, **slower** clock speed, **flexible** instruction set\n",
    "\n",
    "GPUs: **even more** threads, **even slower** clock speed, **limited** instruction set\n",
    "\n",
    "Hopefully, you have an idea about the structure for the system in your computer as a result of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. System Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that previously we called the CPU the *host* and the GPU the *device*, these are the terminology used to refer to these two components in our computers.\n",
    "\n",
    "The CPU is considered the *host* because it queues instructions for the corresponding *device* to follow. The reason is simple, since the CPU operates at much higher clock speeds, it can queue instructions for the *device* to perform at a fast enough rate. \n",
    "\n",
    "The GPU is considered the *device* because it takes instructions from the *host* and distributes them among its multiple threads.\n",
    "\n",
    "However, the structure of the GPU threads are further organized in comparison to CPUs. Typically, threads are organized into *warps* which are clusters of 32 threads that execute simultaneously - with this organization, it is possible to hide latencies due to data transfers or computations.\n",
    "\n",
    "While the organization of threads into warps is meaningful in the context of hiding latencies, it is usually more useful to think about the organization of the threads in the context of your program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. GPU Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the structure of the system, it should follow that there is a means to access data on the *device(s)* \n",
    "\n",
    "**Notes**: The largest latencies in any existing system are memory transfers. Therefore in general, we would like to avoid data transfers whenever possible. They are inevitable in **Real-Time Systems** where data is always handled *\"globally\"*, however the existence of *warps* help mitigate the latency introduced by these data transfers.\n",
    "\n",
    "A very simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array creation: 0.1471 seconds\n",
      "Array operation: 0.0133 seconds\n",
      "Convolution: 0.0005 seconds\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "tic = time.perf_counter()\n",
    "x_d = cp.asarray(x)\n",
    "toc = time.perf_counter()\n",
    "print(f\"Array creation: {toc - tic:0.4f} seconds\")\n",
    "\n",
    "tic = time.perf_counter()\n",
    "y_d = 2*x_d+1\n",
    "toc = time.perf_counter()\n",
    "print(f\"Array operation: {toc - tic:0.4f} seconds\")\n",
    "\n",
    "y_d = 2*cp.arange(1_000)+1\n",
    "tic = time.perf_counter()\n",
    "z_d = cp.convolve(x_d, y_d)\n",
    "toc = time.perf_counter()\n",
    "print(f\"Convolution: {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this example, it may seem as though the GPU computations are not justified, they actually consume a lot of time in comparison to the CPU operations. Let's now compare a larger scale operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy Array creation: 0.0100 seconds\n",
      "Numpy Array operation: 0.0175 seconds\n",
      "Numpy Convolution: 263.1087 seconds\n",
      "Cupy Array creation: 0.0138 seconds\n",
      "Cupy Array operation: 0.0002 seconds\n",
      "Cupy Convolution: 0.0035 seconds\n"
     ]
    }
   ],
   "source": [
    "tic = time.perf_counter()\n",
    "x = np.arange(10_000_000)\n",
    "toc = time.perf_counter()\n",
    "print(f\"Numpy Array creation: {toc - tic:0.4f} seconds\")\n",
    "\n",
    "tic = time.perf_counter()\n",
    "y = 2*x+1\n",
    "toc = time.perf_counter()\n",
    "print(f\"Numpy Array operation: {toc - tic:0.4f} seconds\")\n",
    "\n",
    "y = 2*np.arange(100_000)+1\n",
    "tic = time.perf_counter()\n",
    "z = np.convolve(x, y)\n",
    "toc = time.perf_counter()\n",
    "print(f\"Numpy Convolution: {toc - tic:0.4f} seconds\")\n",
    "\n",
    "tic = time.perf_counter()\n",
    "x_d = cp.asarray(x)\n",
    "toc = time.perf_counter()\n",
    "print(f\"Cupy Array creation: {toc - tic:0.4f} seconds\")\n",
    "\n",
    "tic = time.perf_counter()\n",
    "y_d = cp.asarray(y)\n",
    "toc = time.perf_counter()\n",
    "print(f\"Cupy Array operation: {toc - tic:0.4f} seconds\")\n",
    "\n",
    "y_d = 2*cp.arange(100_000)+1\n",
    "tic = time.perf_counter()\n",
    "z_d = cp.convolve(x_d, y_d)\n",
    "toc = time.perf_counter()\n",
    "print(f\"Cupy Convolution: {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data transfer clearly introduces a large latency in the computation, but the reduction in complex computing is *massive*, over 300x more efficient (based on this example). Clearly, if the main latency is data transfers, then complex computations that are relevant to Machine Learning and Signal Processing can be significantly reduced by using GPUs. \n",
    "\n",
    "Before moving onto the most important part of using GPUs let's show how to transfer data back to the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array transfer (x): 0.0258 seconds\n",
      "Array transfer (y): 0.0003 seconds\n",
      "Array transfer (z): 0.0166 seconds\n"
     ]
    }
   ],
   "source": [
    "tic = time.perf_counter()\n",
    "x_host = cp.asnumpy(x_d)\n",
    "toc = time.perf_counter()\n",
    "print(f\"Array transfer (x): {toc - tic:0.4f} seconds\")\n",
    "\n",
    "tic = time.perf_counter()\n",
    "y_host = cp.asnumpy(y_d)\n",
    "toc = time.perf_counter()\n",
    "print(f\"Array transfer (y): {toc - tic:0.4f} seconds\")\n",
    "\n",
    "tic = time.perf_counter()\n",
    "z_host = cp.asnumpy(z_d)\n",
    "toc = time.perf_counter()\n",
    "print(f\"Array transfer (z): {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. GPU Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen how to transfer data between the *host* and the *device*, we will dive into the main function of GPU Accelerated Computing.\n",
    "\n",
    "In research, there are many times when you will need to implement your own new method. This new method, assuming that it is associated with **Signal Processing** or **Machine Learning**, will often operate on *vectors*, *matrices*, or *tensors* of all shapes and sizes, meaning that there can be a pattern-like memory access and possibility for Parallelization.\n",
    "\n",
    "Here we will show an example of a kernel and express the key considerations when writing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import math\n",
    "\n",
    "@cuda.jit\n",
    "def randomFeatureMap(x, y, z, N, M, K):\n",
    "    thidx = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    thidy = cuda.threadIdx.y + cuda.blockIdx.y * cuda.blockDim.y\n",
    "    thidz = cuda.threadIdx.z + cuda.blockIdx.z * cuda.blockDim.z\n",
    "    if thidx < N and thidy < M and thidz < K:\n",
    "        z[thidx, thidy, thidz] = math.exp(x[thidx]/20) * math.log(y[thidy])\n",
    "    cuda.syncthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we built a fully functional kernel which generates a random FeatureMap, which is common in any machine learning or signal processing process.\n",
    "\n",
    "Clearly, x and y are the 2 input vectors, and z is the 1 output tensor.\n",
    "\n",
    "N, M and K are inputs that specify the shape of the corresponding input vector and the output tensor. It is possible to simply extract these values by using numba's built-in functions, but it is common practice the values in as an argument.\n",
    "\n",
    "In a kernel, thidx, thidy and thidz are used to access input objects in a patterned way. It is not necessary to use all 3 indeces, you can use 2 or 1 depending on the program you are writing. \n",
    "\n",
    "Under extremely careful construction, it is likely that the if statement to check that the indeces are being correctly bounded are not necessary, however it is standard practice to code this restriction.\n",
    "\n",
    "You may have noticed that each threadid is expressed as a sum of the threadIdx and some *block* information. The threads are the corresponding threads on the GPU which we have already discussed, however the blocks are clusters of threads, expressing how to \"sparsify\" the operations for the corresponding output. For very large matrices, it is clear that 1024 threads (the typical amount of threads in a GPU) will not suffice, which is why we need to *blockify* the operations for the output.\n",
    "\n",
    "If you recall, *warps* are also clusters of threads. The main distinction between blocks and warps is that blocks simply express the total amount of resources that will be necessary to perform a kernel, whereas warps are the physical clusters of 32 threads to be ran immediately and independently from other warps.\n",
    "\n",
    "Finally ```cuda.syncthreads()``` is a function which will force the GPU to wait for all the threads to finish computing before moving on. This is usually only necessary for more complex kernels with sequential operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. GPU System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have discussed all the main tools necessary to create a system that uses a GPU. Let's see how we would use one in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU time: 0.0311 seconds\n",
      "CPU time: 0.4961 seconds\n",
      "Transfer latency: 0.0310 seconds\n",
      "Speedup: 15.9352 times\n",
      "GPU == CPU: True\n"
     ]
    }
   ],
   "source": [
    "#We are setting the seed to make the results reproducible for everyone\n",
    "np.random.seed(42)\n",
    "#We are creating random data, consider that in a real time system we would just have the data,\n",
    "#but for synthetic data we would have to generate it here\n",
    "\n",
    "#First let's set the data type and dimensions of the data\n",
    "N = 2**16\n",
    "M = 32\n",
    "K = 16\n",
    "x = np.empty((N), dtype=np.float32)\n",
    "y = np.empty((M), dtype=np.float32)\n",
    "\n",
    "#Now we fill the data with random values\n",
    "x = np.random.rand(N)*16\n",
    "y = np.random.rand(M)*16\n",
    "\n",
    "#Now, we need to allocate memory in the GPU\n",
    "tic1 = time.perf_counter()\n",
    "x_d = cuda.to_device(x)\n",
    "y_d = cuda.to_device(y)\n",
    "z_d = cuda.device_array((N,M,K), dtype=np.float32)\n",
    "\n",
    "#Now we create the output array\n",
    "tic2 = time.perf_counter()\n",
    "randomFeatureMap[(N//2, 1, 1), (2, M, K)](x_d, y_d, z_d, N, M, K)\n",
    "cuda.synchronize()\n",
    "\n",
    "#Now we transfer the data back to the host\n",
    "tic3 = time.perf_counter()\n",
    "z_gpu = z_d.copy_to_host()\n",
    "tic4 = time.perf_counter()\n",
    "\n",
    "#We can optionally print the results\n",
    "#print(z)\n",
    "\n",
    "#Instead, We'd like to emphasize that the computing time is low and accurate\n",
    "z_temp = np.empty((N,M), dtype=np.float32)\n",
    "z_cpu = np.empty((N,M,K), dtype=np.float32)\n",
    "tic5 = time.perf_counter()\n",
    "for i in range(N):\n",
    "    for j in range(M):\n",
    "        z_temp[i,j] = math.exp(x[i]/20) * math.log(y[j])\n",
    "z_cpu = np.repeat(z_temp[:,:,np.newaxis], K, axis=2)\n",
    "tic6 = time.perf_counter()\n",
    "\n",
    "#Let's compare the results\n",
    "print(f\"GPU time: {tic4-tic1:0.4f} seconds\")\n",
    "print(f\"CPU time: {tic6-tic5:0.4f} seconds\")\n",
    "#print(f\"Transfer latency: {(tic2-tic1)+(tic4-tic3):0.4f} seconds\")\n",
    "print(f\"Speedup: {(tic6-tic5)/(tic4-tic1):0.4f} times\")\n",
    "\n",
    "print('GPU == CPU:', np.allclose(z_gpu, z_cpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just observed (depending on your machine) around or over 10x speedup in computing, and this is for a singular computation. This is clearly very good for scaling computing.\n",
    "\n",
    "While the results are great and there are clear implications about this speedup, let's dive into the *kernel configuration*.\n",
    "\n",
    "Typically function calls are made where you just call \n",
    "\n",
    "```function(...)``` \n",
    "\n",
    "However, kernels are called \n",
    "\n",
    "```kernel[(grid_dims),(block_dims)]```\n",
    "\n",
    "Again returning to the concept of structure, *threads* are organized into *blocks*, and *blocks* are organized into a *grid*.\n",
    "\n",
    "As mentioned previously, there are many occasions where a block does not iterate over the entire object, which is why a *grid* is necessary to express how to iterate through the object, in terms of the blocks.\n",
    "\n",
    "Finally, the ```cuda.synchronize()``` function is necessary to ensure that kernels that depend on previous results do not begin until the necessary results have been generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Minimizing Latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is mostly for completeness, but so we will not fully dive into this topic until the sequel workshop, *Hardware Accelerated Scientific Computing* (stay tuned). However we will mention key topics for consideration.\n",
    "\n",
    "We've already seen an example of a *full system* that makes use of the GPU device, however we have only scratched the surface when it comes to optimizing the GPU's performance. \n",
    "\n",
    "It is possible that an implementation like in our example is the most optimal one, especially in cases where *a lot* of data is handled simultaneously. This mostly depends on your GPU's potential which you can check by running ```nvidia-smi``` in your terminal, however typically this is not the case.\n",
    "\n",
    "There are ways to quantify the efficiency of your program, namely the *compute to global memory access ratio* (CGMA) which is defined as floating-point calculations : global memory access. \n",
    "\n",
    "Some intuition for this: High-End GPUs today have *memory-bandwidth* supporting between 500 and 1000 GB/s. On the other hand, they support between 600 and 1300 TFLOPS, that is Terra *Floating Point Operations*. This implies that **optimal** *CGMA* ratios should hang around 1000. \n",
    "\n",
    "There are a few ways to hide latency, one which was already addressed (refer to the warps details above), and the other is to build programs with strong consideration for memory transfers/access. A memory transfer from the CPU to the GPU is inevitable, however by making use of the GPU's local resources composed of *registers* and *shared-memory*, it is possible to further reduce memory latency.\n",
    "\n",
    "We won't dive into access patterns yet, but let's define different memory types. *Device Global Memory* is the GPU's accomadated memory, it has the most amount of memory, implemented using DDR4, it is shared amongst all threads and it is the slowest of the 3. *Device Shared Memory* is the GPU's local memory which is shared amongst threads in a block - this means that as its name implies, the memory is shared amongst these threads, and it is faster than *Device Global Memory*. *Device Local Memory* is also the GPU's local memory which is private amongst the threads in a block, it can only be accessed by its own thread and is the fastest of the memory accesses.\n",
    "\n",
    "In creating kernels, these are important considerations, however libraries like CuPy (in the next section), take care of most of this for us already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. NumPy (CPU) and CuPy (GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it can be a fun (and humbling) learning experience to implement kernels in Numba, we'd like in general to avoid making them from scratch because super corporations like NVIDIA have already implemented most Linear Algebra, Statistics or Machine Learning tools that you would ever need.\n",
    "\n",
    "My intent is to just introduce some tools/functions that will accelerate your workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```cp.as_array(x)``` sends a numpy array to the device\n",
    "\n",
    "```cp.to_device(x_d)``` sends a device array to the host\n",
    "\n",
    "```cp.empty((N,M,K), datatype=cp.float32)``` allocates a device array\n",
    "\n",
    "```cp.zeros((N,M,K), datatype=cp.float32)``` instantiates a device array with 0s\n",
    "\n",
    "```cp.repeat(X_d, K, axis=a)``` repeats a device array K times along axis a\n",
    "\n",
    "```X_d@W_d``` performs a matrix multiplication\n",
    "\n",
    "```X_d(*)W_d``` performs an element-wise operation (adding,subtracting, etc.)\n",
    "\n",
    "CuPy and NumPy have very similar workflows, so actually most functions will work nearly identically as long as you specify to use NumPy instead:\n",
    "\n",
    "```np.empty((N,M,K), datatype=np.float32)``` allocates a host array\n",
    "\n",
    "```np.zeros((N,M,K), datatype=np.float32)``` instantiates a host array with 0s\n",
    "\n",
    "```np.repeat(X, K, axis=a)``` repeats a host array K times along axis a\n",
    "\n",
    "```X@W``` performs a matrix multiplication\n",
    "\n",
    "```X(*)W``` performs an element-wise operation (adding,subtracting, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU computing is important because it enables the rapid processing of large-scale computations and data parallelism, significantly accelerating tasks like machine learning, scientific simulations, and image rendering. This enhances performance and efficiency in various fields, including AI, research, and gaming.\n",
    "\n",
    "We covered topics including: memory management, kernel development and configuration, and workflow acceleration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SPS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
